{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_file = 'images_background'\n",
    "test_file = 'images_evaluation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = 'https://raw.githubusercontent.com/brendenlake/omniglot/master/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting images_background ... File already downloaded.. Assuming those are extracted too..\n",
      "Getting images_evaluation ... File already downloaded.. Assuming those are extracted too..\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_omniglot():\n",
    "    \n",
    "    for file_name in ['images_background', 'images_evaluation']:\n",
    "        print(f'Getting {file_name} ...', end=' ')\n",
    "        # creating directory if it is not there\n",
    "        file_path = os.path.abspath(Path(f'omniglot/{file_name}.zip'))\n",
    "        if os.path.isfile(file_path):\n",
    "            print('File already downloaded.. Assuming those are extracted too..')\n",
    "            continue\n",
    "        dir_name = os.path.dirname(file_path)\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "\n",
    "        # download the file to 'omniglot' directory\n",
    "        dataset_path = tf.keras.utils.get_file(fname=file_path,\n",
    "                                               origin=f'{BASE_PATH}/{file_name}.zip',\n",
    "                                               extract=False)\n",
    "\n",
    "        # extracting to 'omniglot' directory\n",
    "        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('./omniglot/')\n",
    "\n",
    "# get the dataset from Github\n",
    "get_omniglot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Omniglot Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'omniglot'\n",
    "train_path = os.path.join(dataset_path, 'images_background')\n",
    "validation_path = os.path.join(dataset_path, 'images_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "omniglot = Path(dataset_path)\n",
    "alphabets = list(omniglot.glob('images_background/*'))\n",
    "number_of_alphabets = len(alphabets)\n",
    "\n",
    "# shuffle and split the dataset\n",
    "random.seed(25)\n",
    "random.shuffle(alphabets)\n",
    "train_split = 0.8\n",
    "train_alphabets = alphabets[:int(number_of_alphabets*train_split)]\n",
    "val_alphabets = alphabets[int(number_of_alphabets*train_split):]\n",
    "eval_alphabets = list(omniglot.glob('images_evaluation/*'))\n",
    "\n",
    "# conver them from WIndowsPath to string\n",
    "train_alphabets = [alpha for alpha in train_alphabets]\n",
    "val_alphabets = [alpha for alpha in val_alphabets]\n",
    "eval_alphabets = [alpha for alpha in eval_alphabets]\n",
    "\n",
    "\n",
    "# combine all alphabets to get the dictionary\n",
    "alphabets = train_alphabets + val_alphabets + eval_alphabets\n",
    "\n",
    "# load the folder structure into dictionary\n",
    "alphabets_dict = {}\n",
    "for alphabet in alphabets:\n",
    "    # get images from each character directory\n",
    "    alphabets_dict[alphabet.name] = {\n",
    "        character.name: [image.name for image in character.glob('*')]\n",
    "        for character in alphabet.glob('*')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_two_pairs(alphabets):\n",
    "    \"\"\"\n",
    "    Generater to get pairs of images for siamese network\n",
    "    \"\"\"\n",
    "    for alphabet in alphabets:\n",
    "        alphabet = Path(alphabet.decode())\n",
    "        # select two characters from the current alhabet\n",
    "        char1, char2 = random.sample(alphabets_dict[alphabet.name].keys(), 2)\n",
    "        \n",
    "        # select 3 images from char1 and 1 image from char2\n",
    "        same_images = random.sample(alphabets_dict[alphabet.name][char1], 3)\n",
    "        different_image = random.choice(alphabets_dict[alphabet.name][char2])\n",
    "        \n",
    "        # (char1, char1) of same class \n",
    "        yield (\n",
    "            # images pair\n",
    "            os.path.join(alphabet, char1, same_images[0]),\n",
    "            os.path.join(alphabet, char1, same_images[1]),\n",
    "            # label \n",
    "            1\n",
    "        )\n",
    "        \n",
    "        # and (char1, char2) of different class\n",
    "        yield (\n",
    "            # images pair\n",
    "            os.path.join(alphabet, char1, same_images[2]),\n",
    "            os.path.join(alphabet, char2, different_image),\n",
    "            # label \n",
    "            0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_tfds(alphabets):\n",
    "    return tf.data.Dataset.from_generator(\n",
    "    generator = get_two_pairs,\n",
    "    output_types=(tf.string, tf.string, tf.int32),\n",
    "    args=([str(alpha) for alpha in alphabets],)\n",
    ")\n",
    "\n",
    "# train, validation and  test(eval dataset)\n",
    "train_ds = get_tfds(train_alphabets)\n",
    "val_ds = get_tfds(val_alphabets)\n",
    "eval_ds = get_tfds(eval_alphabets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'omniglot\\\\images_background\\\\Ojibwe_(Canadian_Aboriginal_Syllabics)\\\\character07\\\\0843_01.png' \n",
      " b'omniglot\\\\images_background\\\\Ojibwe_(Canadian_Aboriginal_Syllabics)\\\\character07\\\\0843_07.png'\n",
      "1\n",
      "\n",
      "b'omniglot\\\\images_background\\\\Ojibwe_(Canadian_Aboriginal_Syllabics)\\\\character07\\\\0843_10.png' \n",
      " b'omniglot\\\\images_background\\\\Ojibwe_(Canadian_Aboriginal_Syllabics)\\\\character13\\\\0849_16.png'\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(25)\n",
    "for image1, image2, label in train_ds.take(2):\n",
    "    print(image1.numpy(),'\\n',image2.numpy())\n",
    "    print(label.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 105\n",
    "IMG_WIDTH = 105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def plot_siamese(image1, image2, label):\n",
    "    fig, axarr = plt.subplots(1, 2)\n",
    "    fig.suptitle(\"Different\" if label==0 else \"Same\", fontsize=16)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(image2)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def read_images(file_path1, file_path2, label):\n",
    "    def read_image(file_path):\n",
    "        # read the image from file as raw string\n",
    "        img = tf.io.read_file(filename=file_path)\n",
    "\n",
    "        # convert the string image to 3D uint8 tensor\n",
    "        img = tf.image.decode_jpeg(contents=img, channels=3)\n",
    "\n",
    "        # convert that image to float32\n",
    "        img = tf.image.convert_image_dtype(image=img, dtype=tf.float32)\n",
    "        \n",
    "        return tf.image.resize(images=img, size=[IMG_HEIGHT, IMG_WIDTH])\n",
    "    \n",
    "    \n",
    "    return read_image(file_path1), read_image(file_path2), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = val_ds.map(read_images, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "for image1, image2, label in ds.take(2):\n",
    "    plot_siamese(image1, image2, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(ds):\n",
    "    \n",
    "    # read images from strings\n",
    "    ds = ds.map(read_images, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    # repeat the train dataset hoping that all the classes are\n",
    "    # covered for an alphabet.\n",
    "    ds = ds.repeat(count=32)\n",
    "    \n",
    "    # to train the model in batches\n",
    "    ds = ds.batch(batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # prefetch: called after batching so that batches are\n",
    "    # pre-fetched in background while training\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# prepare dataset for training, validation and testing(evaluating)\n",
    "train_ds_final = prepare_dataset(train_ds)\n",
    "val_ds_final = prepare_dataset(val_ds)\n",
    "eval_ds_final = prepare_dataset(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 105, 105, 3) (32, 105, 105, 3)\n"
     ]
    }
   ],
   "source": [
    "for image1s, image2s, labels in train_ds_final.take(1):\n",
    "    print(image1s.shape, image2s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADkCAYAAAA7Ove+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAJA0lEQVR4nO3de6j8+RzH8debHyshcolkW1JSav0htbHsn3Irm1v+0C9yaUuR/EFhU2j5gxVFySV2lbUtEjb+WJQ7u27rktXPuqyItftz21Iff8z3Z8c4+zu3Oe85M/N41GnOOTPfme/5/eY8z/t85jtzaowRAHrcbdU7ALBNRBegkegCNBJdgEaiC9BIdAEaiS53qapOVtWYe/t7VZ2qqmuq6vlVdbe5y543XebkwnW8oapurqp/V9UN0+ceWlWfraq/TNu8uvlL25Pp63/JqveDzXJi1TvAWnhekt8mOSfJuUmekeQTSV5eVc8aY/wzyS1JLkhy05mNquqJSd6a5J1JPp3k9HTWm5I8NcnJabtTHV/EAZzM7HvkQyveDzaI6LIXN4wxfjn38ceq6qokVyV5R5JXjTHuSPLNhe0eO52+f4zxq4XP/2CMcc0ydq6qzpluH449ywscyBjj6iSfSfKyqrr34vJCVV2X5CPTxW+azvtIVY0kFyW5cG7Z4rxpm0dW1RVV9aequqOqbqiq58zfblVdOm3zuKq6tqr+luSTc+dfXFXfrKp/VNVfq+qqqjp34TpOVdXHq+qFVfXTadnku1X15LnLXJfZNP6kuf28bmn/gGwt0eUwPp/ZksMTdjjvkiRvn96/OLOlhzdPpz9Mcv30/gVJbqmqRyT5VpLzk7wmybOTfD/J1VX17B2u/zNJvjJd7l1JUlWvTHJ1khuTPDfJK5I8LslXquq+C9tfmOS1Sd6Y5AVJ7p7kc1V1/7n9v37a1zP7ecke/k3grCwvcBg3T6cPm3s/STLGuLGqziwpXD/GODW9/+uqOp3k32OM/y5HVNWlSSrJU8cYf54+fe0U47ck+ezCbb9njHH53Pb3SXJZkg+PMV4y9/lvJflFkpcmeffc9vdL8vgxxq3T5f6Q5DtJnp7kymn/b09yYn4/4bBMuhxGTafLeNWkp2U2Od9WVSfOvCW5Nsn5VXW/hcsvrgdfkFlIr1jY/rdJfpbkKQuX/8aZ4E5+NJ2eGzhCJl0O4xHT6S1LuK6HJHnx9LaTBya5fe7jxdt8yHT65bvY/taFj/8y/8EY446qSpJ77bqncAiiy2E8I8m/knwvd0bvoP6c5GuZLRHs5PcLHy9O12eWJE4m+ckO25/e4XPQTnQ5kKq6OLMHsS4fY/xjmhIP44uZLRH8ZDrud7++nllYHz3G+Ohhd2ZyR5LFB+DgUESXvXh8VT0oyT0zW/N8ZmZPmPhSktcv6TbelOTbSb5aVe/N7AkTD8js6INHzT84tpMxxu1V9bok76uqByf5QpLbkjw8s0O/rhtjXLnPfboxySVV9YLMnvRxeozx831eB/wP0WUvrppO/5Xkj5kdyvXCJJ8aS/rTI2OMm6vqCUkuTfK2JA/ObMngx0n2NLmOMT5QVb9J8rokL0pyjyS/S/LVJDccYLcuS/KYJB9Mcp/MDlG76ADXA/9V/lwPQB+HjAE0El2ARqIL0Eh0ARqJLkAj0QVoJLoAjUQXoJHoAjQSXYBGogvQSHQBGokuQCPRBWgkugCNRBegkegCNBJdgEaiC9BIdAEaiS5AI9EFaCS6AI1EF6CR6AI0El2ARqIL0Eh0ARqJLkAj0QVoJLoAjUQXoJHoAjQSXYBGogvQSHQBGokuQCPRBWgkugCNRBegkegCNBJdgEaiC9BIdAEaiS5AI9EFaCS6AI1EF6CR6AI0OrHL+aNlL9hmtaLbdd/mqO143zbpAjQSXYBGogvQSHQBGokuQCPRBWgkugCNRBegkegCLaoqVat6LszxIboAjUQXOHIm3Dvt9toLAAcmtv/PpAvQSHSBI2HK3ZnlBWBphHZ3ogsNzhajMTbjpX0Fd28sLwA0MukCh2LC3R+TLnBggrt/ogvQSHRhxdZxWvQ6CgcnunAMiNj2EF2ARqILDfZ6LO46TLvrsI/HmUPGoMmZ8O4WrTPnH7cnTYjtcph0ARqJLjQ7bhPsMm3y17YsogsrMMbYNVDr9uu84O6N6AI0WvsH0tZtGlgGEwXdtvH77KiYdNeQb4DNsZclhuP+/72XpRLuJLoAjUR3Ta3DBMTe7GVK9H+9OdZ+TRe2xSqeNCH2yye6cAzs9dlqi5dZ9Vrqqm9/HVleAGi09pPuNvyk9Ssed6Wqjux7wP3uaKx9dGGTjDH2HbtVLDdsw7BzVCwvADQy6cIxMz9F+hV/85h04Rjb77O9lhHp3Y4Bt7RwOKIL0MjyAqyB/Sw5HNe/PMGMSRfWzFH+vTVryEdPdAEaWV6ANbSKP3JpuWI5TLqwxpa11OBV6/qILkAjywuwJQ7zdGFLC8tj0oU1d9ggWlboZdKFDbCf1+Odv9xuwTbhLp9JF6CRSRc2yH5fLMfSQj+TLmyo47o0sO2hF12ARqILG2y/Lw25uG33bW4D0YUtIILHh+gCNHL0AmyJvR7Layo+WiZd2DKiulqiC9BIdGEL3dURBqbgoye6sMWOMrJnu+5tfv1e0QVo5OgF2HKWFHqZdAEarf2ke7Z1IT/BgePGpAvQaO0n3bPZ1kdHgePLpAscmd1ecWwbByPRBWgkugCNRBegkegCNFr76DoWF1gnG3HI2KaHdxsf4YVNtfaTLsA6EV2ARqIL0Eh0ARqJLkAj0QVoJLoAjUQXoJHoAjQSXYBGogvQSHQBGokuQCPRBWgkugCNRBegkegCNBJdgEaiC9BIdAEaiS5AI9EFaCS6AI1EF6CR6AI0El2ARqIL0Eh0ARqJLkAj0QVoJLoAjUQXoJHoAjQSXWClqipVterdaCO6wJEbY6x6F44N0QVoJLoAjUQXoJHoAjQSXYBGogvQSHQBGokuQCPRBVqMMTxJIqILNFsM77bFWHQBGp1Y9Q5wMNs0GbB5tvn+a9JdAzv9OgasJ9EFaGR5YU2YbmEzmHQBGokuQCPRBWgkugCNRBegkegCNBJdgEaiC9BIdAEaiS5AI9EFaCS6AI1EF6CR6AI0El2ARqIL0Eh0ARqJLkAj0QVoJLoAjUQXoJHoAjQSXYBGogvQSHQBGp3Y5fxq2Qvo577NSph0ARqJLkAj0QVoJLoAjUQXoJHoAjT6D03V8NOvVV9AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image1, image2, label in zip(image1s[:4], image2s[:4], labels[:4]):\n",
    "    plot_siamese(image1, image2,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Siamese Neural Networks for One-shot Image Recognition\n",
    "### https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 105, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_SHAPE = (105, 105, 3)\n",
    "INPUT_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def conv_block(name, filters, kernel_size, l2_reg_factor,):\n",
    "    \"\"\"Same as original paper but with Batch \"\"\"\n",
    "    with tf.name_scope(name=name):\n",
    "        return tf.keras.Sequential(layers=[\n",
    "            layers.Conv2D(filters=filters, kernel_size=kernel_size,\n",
    "                          padding='valid',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(l2_reg_factor)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPool2D(pool_size=(2,2))\n",
    "        ], name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class L1DistanceLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(L1DistanceLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.abs(inputs[0] - inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class Siamese(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(Siamese, self).__init__(**kwargs)\n",
    "        \n",
    "        # CNN encoder\n",
    "        self.encoder = tf.keras.Sequential(layers=[\n",
    "            conv_block(name='Conv1', filters=64, kernel_size=(10,10), l2_reg_factor=1e-2),\n",
    "            conv_block(name='Conv2', filters=128, kernel_size=(7,7), l2_reg_factor=1e-2),\n",
    "            conv_block(name='Conv3', filters=128, kernel_size=(4,4), l2_reg_factor=1e-2),\n",
    "            conv_block(name='Conv4', filters=256, kernel_size=(4,4), l2_reg_factor=1e-2),\n",
    "            layers.Flatten()\n",
    "        ], name='encoder')\n",
    "        \n",
    "        # dense layer\n",
    "        self.dense = tf.keras.layers.Dense(units=4096, activation='sigmoid',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                          name='dense')\n",
    "        # l1-distance layer\n",
    "        self.l1_distance = L1DistanceLayer(name='distance_layer')\n",
    "        \n",
    "        # output layer\n",
    "        self.logits = tf.keras.layers.Dense(units=1, activation=None)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        # flattened outputs of both images\n",
    "        encoded_image1s = self.dense(self.encoder(inputs[0], training=training))\n",
    "        encoded_image2s = self.dense(self.encoder(inputs[1], training=training))\n",
    "        \n",
    "        # get l1-distance between those images and return the logits\n",
    "        return self.logits(self.l1_distance(inputs=(encoded_image1s, encoded_image2s)))\n",
    "\n",
    "    \n",
    "    def compile(self, loss_fn, optimizer):\n",
    "        \"\"\"Initialize loss_fn and the optimizer\"\"\"\n",
    "        self.loss_fn, self.optimizer = loss_fn, optimizer\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"returns: loss function and optimizer and acc_fn for this model\"\"\"\n",
    "        return {\n",
    "            \"loss_fn\": self.loss_fn,\n",
    "            \"optimizer\": self.optimizer,\n",
    "        }\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, batch, labels, train_loss, train_acc):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # recored all the values on a tape in forward pass\n",
    "            logits = self.call(batch, training=True)\n",
    "            loss = self.loss_fn(y_true=labels, y_pred=logits)\n",
    "\n",
    "        # backward pass : grad of parameters w.r.t Loss\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(grads_and_vars=zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)  # Mean Metric : Just update the loss\n",
    "        train_acc(y_true=labels, y_pred=tf.nn.sigmoid(logits))  # accuracy Metric\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, batch, labels, test_loss, test_acc):\n",
    "        \"\"\" Run the Inference for the given batch and update the metrics \"\"\"\n",
    "        \n",
    "        logits = self.call(batch, training=False)\n",
    "        loss = self.loss_fn(y_true=labels, y_pred=logits)\n",
    "        \n",
    "        test_loss(loss)  # Mean Metric - just update the loss\n",
    "        test_acc(y_true=labels, y_pred=tf.nn.sigmoid(logits))  # Accuracy Metric\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = Siamese(name='Siamese')\n",
    "# update the loss function and optiizer for the model\n",
    "model.compile(loss_fn=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.optimizers.SGD(nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train and test(validation) metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='validation_loss')\n",
    "val_acc = tf.keras.metrics.BinaryAccuracy(name='validation_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Creating & loading check points for this model\n",
    "- https://www.tensorflow.org/guide/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'iter:0' shape=() dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.iterations  # before loading checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# define model checkpoint with objects that we want to save\n",
    "# as attriutes to the checkpoint object\n",
    "model_ckpt = tf.train.Checkpoint(step=tf.Variable(0), epoch=tf.Variable(0),\n",
    "                                 optimizer=model.optimizer,\n",
    "                                 model=model)\n",
    "\n",
    "# manager to manage multiple checkpoints. Here, we keep only lateset 10 model check points\n",
    "model_ckpt_manager = tf.train.CheckpointManager(model_ckpt, './siamese_tf_ckpts',\n",
    "                                                max_to_keep=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./siamese_tf_ckpts\\ckpt-75\n"
     ]
    }
   ],
   "source": [
    "# load the model (latest checkpoint)\n",
    "# load the model check point\n",
    "model_ckpt.restore(model_ckpt_manager.latest_checkpoint)\n",
    "\n",
    "if model_ckpt_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(model_ckpt_manager.latest_checkpoint))\n",
    "else:\n",
    "     print(\"Training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 150)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt.step.numpy(), model_ckpt.epoch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'iter:0' shape=() dtype=int64, numpy=7200>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.iterations  # After loading checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=150>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=7200>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt.epoch, model_ckpt.step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Creating summary writers for this model\n",
    "wtrain = tf.summary.create_file_writer(logdir=\"logs/siamese/train\")\n",
    "wtest = tf.summary.create_file_writer(logdir=\"logs/siamese/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:151/200 loss:0.22218  acc:0.9251  val_loss:0.38451  val_acc:0.8255 - 14s(total: 14s)\n",
      "\n",
      "Epoch:152/200 loss:0.20316  acc:0.9310  val_loss:0.35224  val_acc:0.8750 - 12s(total: 27s)\n",
      "Saved checkpoint for step 7300: ./siamese_tf_ckpts\\ckpt-76\n",
      "\n",
      "Epoch:153/200 loss:0.20110  acc:0.9368  val_loss:0.40362  val_acc:0.8333 - 13s(total: 39s)\n",
      "\n",
      "Epoch:154/200 loss:0.21622  acc:0.9238  val_loss:0.29969  val_acc:0.8698 - 9s(total: 48s)\n",
      "Saved checkpoint for step 7400: ./siamese_tf_ckpts\\ckpt-77\n",
      "\n",
      "Epoch:155/200 loss:0.19620  acc:0.9329  val_loss:0.30878  val_acc:0.8672 - 13s(total: 61s)\n",
      "\n",
      "Epoch:156/200 loss:0.19634  acc:0.9329  val_loss:0.32814  val_acc:0.8620 - 10s(total: 71s)\n",
      "Saved checkpoint for step 7500: ./siamese_tf_ckpts\\ckpt-78\n",
      "\n",
      "Epoch:157/200 loss:0.20709  acc:0.9310  val_loss:0.42142  val_acc:0.8255 - 12s(total: 83s)\n",
      "\n",
      "Epoch:158/200 loss:0.20801  acc:0.9349  val_loss:0.36295  val_acc:0.8438 - 8s(total: 91s)\n",
      "Saved checkpoint for step 7600: ./siamese_tf_ckpts\\ckpt-79\n",
      "\n",
      "Epoch:159/200 loss:0.17453  acc:0.9421  val_loss:0.37449  val_acc:0.8438 - 9s(total: 100s)\n",
      "\n",
      "Epoch:160/200 loss:0.20731  acc:0.9277  val_loss:0.36837  val_acc:0.8307 - 8s(total: 109s)\n",
      "Saved checkpoint for step 7700: ./siamese_tf_ckpts\\ckpt-80\n",
      "\n",
      "Epoch:161/200 loss:0.19745  acc:0.9382  val_loss:0.36643  val_acc:0.8542 - 9s(total: 118s)\n",
      "\n",
      "Epoch:162/200 loss:0.18313  acc:0.9329  val_loss:0.35280  val_acc:0.8542 - 8s(total: 126s)\n",
      "Saved checkpoint for step 7800: ./siamese_tf_ckpts\\ckpt-81\n",
      "\n",
      "Epoch:163/200 loss:0.21430  acc:0.9310  val_loss:0.39275  val_acc:0.8490 - 9s(total: 135s)\n",
      "\n",
      "Epoch:164/200 loss:0.18674  acc:0.9388  val_loss:0.37884  val_acc:0.8125 - 8s(total: 144s)\n",
      "Saved checkpoint for step 7900: ./siamese_tf_ckpts\\ckpt-82\n",
      "\n",
      "Epoch:165/200 loss:0.20213  acc:0.9329  val_loss:0.37520  val_acc:0.8255 - 9s(total: 153s)\n",
      "\n",
      "Epoch:166/200 loss:0.18223  acc:0.9395  val_loss:0.31814  val_acc:0.8698 - 9s(total: 161s)\n",
      "Saved checkpoint for step 8000: ./siamese_tf_ckpts\\ckpt-83\n",
      "\n",
      "Epoch:167/200 loss:0.19319  acc:0.9375  val_loss:0.33464  val_acc:0.8568 - 9s(total: 171s)\n",
      "\n",
      "Epoch:168/200 loss:0.18748  acc:0.9388  val_loss:0.33132  val_acc:0.8646 - 9s(total: 180s)\n",
      "Saved checkpoint for step 8100: ./siamese_tf_ckpts\\ckpt-84\n",
      "\n",
      "Epoch:169/200 loss:0.18874  acc:0.9368  val_loss:0.37745  val_acc:0.8516 - 10s(total: 190s)\n",
      "\n",
      "Epoch:170/200 loss:0.18316  acc:0.9368  val_loss:0.39128  val_acc:0.8568 - 9s(total: 198s)\n",
      "Saved checkpoint for step 8200: ./siamese_tf_ckpts\\ckpt-85\n",
      "\n",
      "Epoch:171/200 loss:0.18559  acc:0.9408  val_loss:0.33258  val_acc:0.8438 - 9s(total: 208s)\n",
      "\n",
      "Epoch:172/200 loss:0.17587  acc:0.9401  val_loss:0.37560  val_acc:0.8438 - 8s(total: 216s)\n",
      "Saved checkpoint for step 8300: ./siamese_tf_ckpts\\ckpt-86\n",
      "\n",
      "Epoch:173/200 loss:0.18927  acc:0.9349  val_loss:0.35548  val_acc:0.8568 - 11s(total: 227s)\n",
      "\n",
      "Epoch:174/200 loss:0.17381  acc:0.9466  val_loss:0.39299  val_acc:0.8516 - 8s(total: 235s)\n",
      "Saved checkpoint for step 8400: ./siamese_tf_ckpts\\ckpt-87\n",
      "\n",
      "Epoch:175/200 loss:0.17238  acc:0.9447  val_loss:0.40107  val_acc:0.8646 - 9s(total: 244s)\n",
      "\n",
      "Epoch:176/200 loss:0.18500  acc:0.9421  val_loss:0.36443  val_acc:0.8542 - 8s(total: 252s)\n",
      "\n",
      "Epoch:177/200 loss:0.18852  acc:0.9323  val_loss:0.42738  val_acc:0.8229 - 8s(total: 261s)\n",
      "Saved checkpoint for step 8500: ./siamese_tf_ckpts\\ckpt-88\n",
      "\n",
      "Epoch:178/200 loss:0.17981  acc:0.9408  val_loss:0.34759  val_acc:0.8594 - 9s(total: 270s)\n",
      "\n",
      "Epoch:179/200 loss:0.16964  acc:0.9499  val_loss:0.38915  val_acc:0.8464 - 8s(total: 278s)\n",
      "Saved checkpoint for step 8600: ./siamese_tf_ckpts\\ckpt-89\n",
      "\n",
      "Epoch:180/200 loss:0.18521  acc:0.9355  val_loss:0.32439  val_acc:0.8672 - 9s(total: 288s)\n",
      "\n",
      "Epoch:181/200 loss:0.17007  acc:0.9427  val_loss:0.34042  val_acc:0.8542 - 9s(total: 296s)\n",
      "Saved checkpoint for step 8700: ./siamese_tf_ckpts\\ckpt-90\n",
      "\n",
      "Epoch:182/200 loss:0.18691  acc:0.9329  val_loss:0.35524  val_acc:0.8385 - 10s(total: 306s)\n",
      "\n",
      "Epoch:183/200 loss:0.16591  acc:0.9434  val_loss:0.34310  val_acc:0.8594 - 9s(total: 314s)\n",
      "Saved checkpoint for step 8800: ./siamese_tf_ckpts\\ckpt-91\n",
      "\n",
      "Epoch:184/200 loss:0.15810  acc:0.9544  val_loss:0.39851  val_acc:0.8464 - 10s(total: 324s)\n",
      "\n",
      "Epoch:185/200 loss:0.18118  acc:0.9336  val_loss:0.29333  val_acc:0.8828 - 9s(total: 333s)\n",
      "Saved checkpoint for step 8900: ./siamese_tf_ckpts\\ckpt-92\n",
      "\n",
      "Epoch:186/200 loss:0.19066  acc:0.9362  val_loss:0.30879  val_acc:0.8620 - 9s(total: 342s)\n",
      "\n",
      "Epoch:187/200 loss:0.18550  acc:0.9362  val_loss:0.38970  val_acc:0.8385 - 8s(total: 350s)\n",
      "Saved checkpoint for step 9000: ./siamese_tf_ckpts\\ckpt-93\n",
      "\n",
      "Epoch:188/200 loss:0.16862  acc:0.9492  val_loss:0.29235  val_acc:0.8776 - 9s(total: 359s)\n",
      "\n",
      "Epoch:189/200 loss:0.15001  acc:0.9538  val_loss:0.39749  val_acc:0.8568 - 9s(total: 368s)\n",
      "Saved checkpoint for step 9100: ./siamese_tf_ckpts\\ckpt-94\n",
      "\n",
      "Epoch:190/200 loss:0.15065  acc:0.9473  val_loss:0.36292  val_acc:0.8385 - 9s(total: 377s)\n",
      "\n",
      "Epoch:191/200 loss:0.15367  acc:0.9544  val_loss:0.36150  val_acc:0.8464 - 8s(total: 386s)\n",
      "Saved checkpoint for step 9200: ./siamese_tf_ckpts\\ckpt-95\n",
      "\n",
      "Epoch:192/200 loss:0.16150  acc:0.9453  val_loss:0.37829  val_acc:0.8385 - 9s(total: 395s)\n",
      "\n",
      "Epoch:193/200 loss:0.14209  acc:0.9603  val_loss:0.40048  val_acc:0.8203 - 8s(total: 403s)\n",
      "Saved checkpoint for step 9300: ./siamese_tf_ckpts\\ckpt-96\n",
      "\n",
      "Epoch:194/200 loss:0.16993  acc:0.9440  val_loss:0.30019  val_acc:0.8854 - 10s(total: 413s)\n",
      "\n",
      "Epoch:195/200 loss:0.16415  acc:0.9499  val_loss:0.36518  val_acc:0.8698 - 11s(total: 424s)\n",
      "Saved checkpoint for step 9400: ./siamese_tf_ckpts\\ckpt-97\n",
      "\n",
      "Epoch:196/200 loss:0.15493  acc:0.9473  val_loss:0.30966  val_acc:0.8776 - 10s(total: 434s)\n",
      "\n",
      "Epoch:197/200 loss:0.15104  acc:0.9570  val_loss:0.32566  val_acc:0.8672 - 11s(total: 445s)\n",
      "Saved checkpoint for step 9500: ./siamese_tf_ckpts\\ckpt-98\n",
      "\n",
      "Epoch:198/200 loss:0.16334  acc:0.9557  val_loss:0.40607  val_acc:0.8542 - 9s(total: 454s)\n",
      "\n",
      "Epoch:199/200 loss:0.15725  acc:0.9505  val_loss:0.35895  val_acc:0.8464 - 8s(total: 463s)\n",
      "Saved checkpoint for step 9600: ./siamese_tf_ckpts\\ckpt-99\n",
      "\n",
      "Epoch:200/200 loss:0.16792  acc:0.9427  val_loss:0.36673  val_acc:0.8516 - 9s(total: 472s)\n",
      "Saved checkpoint at the end of epoch 9600: ./siamese_tf_ckpts\\ckpt-100\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "log_freq = 10  # for every few optimizer iterations. ie., weight updates\n",
    "ckpt_freq = 100 # for every few optimizer iterations. ie., weight updates\n",
    "\n",
    "global_st = perf_counter()  # Counter before training starts\n",
    "\n",
    "# take last epoch trained from model checkpoint\n",
    "last_epoch = model_ckpt.epoch.numpy()\n",
    "for epoch in range(last_epoch, last_epoch+EPOCHS, 1):    \n",
    "    # To count the time\n",
    "    local_st = perf_counter()\n",
    "    \n",
    "    # Reset the metrics at the start of the every epoch\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_acc.reset_states()\n",
    "    \n",
    "    # training loop, It will update the metrics\n",
    "    for image1s, image2s, labels in train_ds_final:\n",
    "        model.train_step(batch=(image1s, image2s), labels=labels,\n",
    "                         train_loss=train_loss, train_acc=train_acc)\n",
    "        \n",
    "        # log the train metrics \n",
    "        if tf.equal(model.optimizer.iterations % log_freq, 0):\n",
    "            with tf.name_scope('Training'):\n",
    "                with wtrain.as_default():\n",
    "                    tf.summary.scalar(\"Loss\", train_loss.result().numpy(),\n",
    "                                      step=model.optimizer.iterations)\n",
    "                    tf.summary.scalar(\"Accuracy\", train_acc.result().numpy(),\n",
    "                                      step=model.optimizer.iterations)\n",
    "                    wtrain.flush()\n",
    "        \n",
    "        # save the model for every ckpt_freq steps\n",
    "        model_ckpt.step.assign_add(1)\n",
    "        if tf.equal(model_ckpt.step % ckpt_freq, 0):\n",
    "            save_path = model_ckpt_manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(model_ckpt.step.numpy(), save_path))\n",
    "\n",
    "    \n",
    "    # test loop, and it will update the metrics as well\n",
    "    for image1s, image2s, labels in val_ds_final:\n",
    "        model.test_step(batch=(image1s, image2s), labels=labels,\n",
    "                        test_loss=val_loss, test_acc=val_acc)\n",
    "    \n",
    "    # Update the model checkpoint epoch\n",
    "    model_ckpt.epoch.assign_add(1)\n",
    "    \n",
    "    # Update the train and validation for this epoch\n",
    "    with tf.name_scope(\"Per_Epoch\"):\n",
    "        \n",
    "        with wtrain.as_default():\n",
    "            tf.summary.scalar(\"Loss\", train_loss.result().numpy(),\n",
    "                              step=model_ckpt.epoch.numpy())\n",
    "            tf.summary.scalar(\"Accuracy\", train_acc.result().numpy(),\n",
    "                              step=model_ckpt.epoch.numpy())\n",
    "            wtrain.flush()\n",
    "\n",
    "        with wtest.as_default():\n",
    "            tf.summary.scalar(\"Loss\", val_loss.result().numpy(),\n",
    "                              step=model_ckpt.epoch.numpy())\n",
    "            tf.summary.scalar(\"Accuracy\", val_acc.result().numpy(),\n",
    "                              step=model_ckpt.epoch.numpy())\n",
    "            wtest.flush()\n",
    "    \n",
    "    \n",
    "    template = \"\\nEpoch:{}/{} loss:{:0.5f}  acc:{:0.4f}  \"\n",
    "    template += \"val_loss:{:0.5f}  val_acc:{:0.4f} - {:0.0f}s(total: {:0.0f}s)\"\n",
    "    print(template.format(epoch+1, last_epoch+EPOCHS,\n",
    "                      train_loss.result().numpy(), train_acc.result().numpy(), \n",
    "                      val_loss.result().numpy(), val_acc.result().numpy(),\n",
    "                      perf_counter()-local_st, perf_counter()-global_st))\n",
    "\n",
    "\n",
    "# Saving model at the end of training\n",
    "model_ckpt.step.assign(model.optimizer.iterations.numpy())\n",
    "save_path = model_ckpt_manager.save()\n",
    "print(\"Saved checkpoint at the end of epoch {}: {}\".format(int(model_ckpt.step), save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37164bittf2conda087c3d58f46c4b629d08798107e7912e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
