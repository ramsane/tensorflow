{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Data  loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "_0zs3TflcAEr"
   },
   "source": [
    "<pre>\n",
    "1. Download all the data in this folder https://drive.google.com/open?id=1Z4TyI7FcFVEx8qdl4jO9qxvxaqLSqoEu. it contains two file both images and labels. The label file list the images and their categories in the following format:\n",
    "            <b>path/to/the/image.tif,category</b>\n",
    "            \n",
    "    where the categories are numbered 0 to 15, in the following order:\n",
    "\n",
    "    <b>0 letter\n",
    "    1 form\n",
    "    2 email\n",
    "    3 handwritten\n",
    "    4 advertisement\n",
    "    5 scientific report\n",
    "    6 scientific publication\n",
    "    7 specification\n",
    "    8 file folder\n",
    "    9 news article\n",
    "    10 budget\n",
    "    11 invoice\n",
    "    12 presentation\n",
    "    13 questionnaire\n",
    "    14 resume\n",
    "    15 memo</b>\n",
    "    \n",
    "2. On this image data, you have to train 3 types of models as given below. You have to split the data into Train and Validation data.\n",
    "\n",
    "3. Try not to load all the images into memory, use the gernarators that we have given the reference notebooks to load the batch of images only during the train data.\n",
    "or you can use this method also\n",
    "<a href='https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1'>https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1</a>\n",
    "\n",
    "<a href='https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c'>https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c</a>\n",
    "\n",
    "\n",
    "4. You are free to choose Learning rate, optimizer, loss function, image augmentation, any hyperparameters. but you have to use the same architechture what we are asking below. \n",
    "\n",
    "5. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "Note: fit_genarator() method will have problems with the tensorboard histograms, try to debug it, if you could not do use histgrams=0 i.e don't include histograms, check the documentation of tensorboard for more information. \n",
    "\n",
    "6. You can check about Transfer Learning in this link - <a href='https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html'>https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\JPG_DATA'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.path.join('.', 'JPG_DATA')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, 'labels_final.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JPG_DATA/data_final/imagesv/v/o/h/voh71d00/509...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JPG_DATA/data_final/imagesl/l/x/t/lxt19d00/502...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JPG_DATA/data_final/imagesx/x/e/d/xed05a00/207...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label\n",
       "0  JPG_DATA/data_final/imagesv/v/o/h/voh71d00/509...      3\n",
       "1  JPG_DATA/data_final/imagesl/l/x/t/lxt19d00/502...      3\n",
       "2  JPG_DATA/data_final/imagesx/x/e/d/xed05a00/207...      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = df.label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- No need for stratified sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.8, random_state=25)\n",
    "test_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 2), (38400, 2), (9600, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Create a data set for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39848</th>\n",
       "      <td>JPG_DATA/data_final/imagesi/i/s/x/isx37c00/207...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32989</th>\n",
       "      <td>JPG_DATA/data_final/imageso/o/h/s/ohs99c00/400...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  label\n",
       "39848  JPG_DATA/data_final/imagesi/i/s/x/isx37c00/207...     10\n",
       "32989  JPG_DATA/data_final/imageso/o/h/s/ohs99c00/400...      6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT, IMAGE_WIDTH = 156, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def prepare_data(df, size, batch_size=32, train=False, cache=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(tensors=(df.path.values, df.label.values))\n",
    "    \n",
    "    def get_image_label(file_path, label):\n",
    "        \"\"\" To get the image and label\"\"\"\n",
    "        # decode string to grayscale image\n",
    "        img = tf.image.decode_jpeg(\n",
    "            contents=tf.io.read_file(file_path), channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, dtype=tf.float32)  # convert to float\n",
    "        img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    # shuffle the data if the dataset is for training\n",
    "    if train:\n",
    "        ds.shuffle(df.shape[0])\n",
    "    \n",
    "    # get image as numpy array and labels and create batches\n",
    "    ds = ds.map(map_func=get_image_label)\n",
    "    ds = ds.batch(batch_size=batch_size)\n",
    "    \n",
    "    # apply cache to the dataset for faster pipelines\n",
    "    if cache:\n",
    "        if isinstance(cache, str):  # cache in files\n",
    "            ds = ds.cache(cache)\n",
    "        else:  # if all the data can fit in the memory\n",
    "            ds = ds.cache()\n",
    "    \n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # prefetch the data\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_ds = prepare_data(df=train_df, size=[IMAGE_HEIGHT, IMAGE_WIDTH],\n",
    "                       batch_size=BATCH_SIZE, train=False, cache='rvl-train.tfcache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "test_ds = prepare_data(df=test_df, size=[IMAGE_HEIGHT, IMAGE_WIDTH],\n",
    "                       batch_size=BATCH_SIZE, train=True, cache='rvl-test.tfcache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38400/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 192 (64, 156, 256, 3) (64,)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[1;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2657\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m         \u001b[1;34m\"IteratorGetNextSync\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   2660\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_iter = iter(train_ds)\n",
    "\n",
    "i = 0\n",
    "points = 0\n",
    "for images, label in train_iter:\n",
    "    i+=1\n",
    "    points += images.shape[0]\n",
    "    print(i, points, images.shape, label.shape, end='\\r')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 256 (64, 156, 256, 3) (64,)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[1;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2657\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m         \u001b[1;34m\"IteratorGetNextSync\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[0;32m   2660\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_iter = iter(test_ds)\n",
    "\n",
    "i = 0\n",
    "points = 0\n",
    "for images, label in test_iter:\n",
    "    i+=1\n",
    "    points += images.shape[0]\n",
    "    print(i, points, images.shape, label.shape, end='\\r')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "XZXpEZtJcAEu"
   },
   "source": [
    "# Model-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "EF12MYu1cAEy"
   },
   "source": [
    "<pre>\n",
    "1. Use <a href='https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16'>VGG-16</a> pretrained network without Fully Connected layers and initilize all the weights with Imagenet trained weights. \n",
    "2. After VGG-16 network without FC layers, add a new Conv block ( 1 Conv layer and 1 Maxpooling ), 2 FC layers and a output layer to classify 16 classes. You are free to choose any hyperparameters/parameters of conv block, FC layers, output layer. \n",
    "3. Final architecture will be <b>INPUT --> VGG-16 without Top layers(FC) --> Conv Layer --> Maxpool Layer --> 2 FC layers --> Output Layer</b>\n",
    "4. Train only new Conv block, FC layers, output layer. Don't train the VGG-16 network. \n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Create summary writers for this model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "wtrain = tf.summary.create_file_writer(logdir=\"logs/1_Model1/train\")\n",
    "wtest = tf.summary.create_file_writer(logdir=\"logs/1_Model1/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class Model1(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model1, self).__init__(**kwargs)\n",
    "        # vgg feature layer\n",
    "        self.feature = VGG16(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3),\n",
    "                             include_top=False,\n",
    "                             weights='imagenet')\n",
    "        \n",
    "        # VGG16 : to avoid training \n",
    "        self.feature.trainable=False\n",
    "        \n",
    "        self.conv_block = Sequential(layers=[\n",
    "            layers.Conv2D(filters=128, kernel_size=3, padding='same'),\n",
    "            layers.MaxPool2D(pool_size=(2,2))\n",
    "        ])\n",
    "        \n",
    "        self.fc_layer = Sequential(layers=[\n",
    "            layers.Dense(units=128, activation='relu'),\n",
    "            layers.Dense(units=16, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # initialize configs to None\n",
    "        self.loss_fn, self.optimizer, self.acc_fn = None, None, None\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        # feature representation of the input\n",
    "        x = self.feature(x)\n",
    "        x = self.conv_block(x)  # additional conv layers\n",
    "        \n",
    "        # flatten the output before passing it to the output layer\n",
    "        x = tf.reshape(x, shape=[x.shape[0], -1])\n",
    "        x = self.fc_layer(x)  # softmax outputs\n",
    "        \n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_\n",
    "    \n",
    "    @metrics.setter\n",
    "    def metrics(self, metrics):\n",
    "        self.metrics_ = metrics\n",
    "    \n",
    "    def compile(self, loss_fn, optimizer):\n",
    "        \"\"\"Initialize loss_fn and the optimizer\"\"\"\n",
    "        self.loss_fn, self.optimizer = loss_fn, optimizer\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"returns: loss function and optimizer and acc_fn for this model\"\"\"\n",
    "        return {\n",
    "            \"loss_fn\": self.loss_fn,\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"acc_fn\" : self.acc_fn\n",
    "        }\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, images, labels, acc_fn=None):\n",
    "        \"\"\"return the loss and accuracy before doing back-propagation\"\"\"\n",
    "        # Forward : record all the computations on a tape\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = self.call(images)  # softmax outputs\n",
    "            loss = self.loss_fn(y_true=labels, y_pred=out)\n",
    "        \n",
    "        # BackwardProp : calculate and update gradients w.r.t loss (dLoss/dGrad)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(grads_and_vars=zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # find the accuracy if required for this batch\n",
    "        if acc_fn:\n",
    "            acc = acc_fn(y_true=labels, y_pred=out)\n",
    "            return loss, acc\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validate(self, data, train=False):\n",
    "        \"\"\"Updates the metric states for the current dataset\"\"\"\n",
    "        self.reset_metrics()  # reset metrics\n",
    "        \n",
    "        for images, labels in data:\n",
    "            out = self.call(images)\n",
    "            # update all the metrics\n",
    "            for metric in self.metrics:\n",
    "                metric(y_true=labels, y_pred=out)\n",
    "        \n",
    "        return [metric.result() for metric in self.metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 8, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      multiple                  589952    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    multiple                  133264    \n",
      "=================================================================\n",
      "Total params: 15,437,904\n",
      "Trainable params: 723,216\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Model1(name=\"Model1\")\n",
    "# just to get the model summary\n",
    "model1.build(input_shape=(1, IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "    out = model1(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 16])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16]), TensorShape([16, 16]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1087, shape=(), dtype=float32, numpy=3.3050504>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(labels, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1099, shape=(), dtype=float32, numpy=3.3050504>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(labels, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model1.compile(loss_fn=loss_fn, optimizer=opt)\n",
    "model1.metrics = [tf.metrics.SparseCategoricalCrossentropy(),\n",
    "                  tf.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating & loading check points for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model1_ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=model1.optimizer, net=model1)\n",
    "model1_ckpt_manager = tf.train.CheckpointManager(model1_ckpt, './model1_tf_ckpts', max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_ckpt.step.assign(model1.optimizer.iterations.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    }
   ],
   "source": [
    "# load the model check point\n",
    "model1_ckpt.restore(model1_ckpt_manager.latest_checkpoint)\n",
    "\n",
    "if model1_ckpt_manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "     print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float32' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f886b76f0ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mwtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mwtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float32' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "log_freq = 10\n",
    "val_log_freq = 100\n",
    "ckpt_freq = 100\n",
    "\n",
    "train_params = {'loss':[], 'accuracy': [], 'epoch':[]}\n",
    "val_params = {'loss':[], 'accuracy': [], 'epoch':[]}\n",
    "loss_arr = []  # this is for every iteration\n",
    "\n",
    "acc_fn = tf.metrics.SparseCategoricalAccuracy()  # for per step accuracy\n",
    "global_st = perf_counter()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    local_st = perf_counter()\n",
    "    # acc_fn.reset_states() #-not required. Caling below for evert val_log_freq\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        loss, acc = model1.train_step(images, labels, acc_fn)\n",
    "        \n",
    "        if tf.equal(model1.optimizer.iterations % log_freq, 0):\n",
    "            with tf.name_scope('training_loss_and_acc'):\n",
    "                with wtrain.as_default():\n",
    "                    tf.summary.scalar(\"loss\", loss, step=model1.optimizer.iterations)\n",
    "                    tf.summary.scalar(\"acc\", acc, step=model1.optimizer.iterations)\n",
    "                    wtrain.flush()\n",
    "        \n",
    "        # store validation loss for every few steps\n",
    "        if tf.equal(model1.optimizer.iterations % val_log_freq, 0):\n",
    "            val_loss, val_acc = model1.validate(test_ds)\n",
    "            with tf.name_scope(\"Train_Test_validtion\"):\n",
    "                with wtrain.as_default():\n",
    "                    tf.summary.scalar(\"loss\", loss.numpy(), step=model1.optimizer.iterations)\n",
    "                    tf.summary.scalar(\"acc\", acc.numpy(), step=model1.optimizer.iterations)\n",
    "                    wtrain.flush()\n",
    "                # reset acc_fn. It will be cumulative till end of epoch otherwise.\n",
    "                acc_fn.reset_states()\n",
    "\n",
    "                with wtest.as_default():\n",
    "                    tf.summary.scalar(\"loss\", val_loss.numpy(), step=model1.optimizer.iterations)\n",
    "                    tf.summary.scalar(\"acc\", val_acc.numpy(), step=model1.optimizer.iterations)\n",
    "                    wtest.flush()\n",
    "                    \n",
    "        # save the model for every ckpt_freq steps\n",
    "        if tf.equal(model1.optimizer.iterations % ckpt_freq, 0):\n",
    "            model1_ckpt.step.assign(model1.optimizer.iterations.numpy())\n",
    "            save_path = manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(int(model1_ckpt.step), save_path))\n",
    "\n",
    "    # update the params\n",
    "    train_params['loss'].append(loss.numpy())\n",
    "    train_params['accuracy'].append(acc.numpy())\n",
    "    train_params['epoch'].append(epoch+1)\n",
    "    \n",
    "    # validating the model after this epoch\n",
    "    val_loss, val_acc = model1.validate(test_ds)\n",
    "    val_params['loss'].append(val_loss.numpy())\n",
    "    val_params['accuracy'].append(val_acc.numpy())\n",
    "    val_params['epoch'].append(epoch+1)\n",
    "    \n",
    "    # write them to the tensorboard\n",
    "    with tf.name_scope(\"per_epoch_params\"):\n",
    "        with wtrain.as_default():\n",
    "            tf.summary.scalar(\"loss\", loss.numpy(), step=epoch+1)\n",
    "            tf.summary.scalar(\"acc\", acc.numpy(), step=epoch+1)\n",
    "            wtrain.flush()\n",
    "\n",
    "        with wtest.as_default():\n",
    "            tf.summary.scalar(\"loss\", val_loss, step=epoch+1)\n",
    "            tf.summary.scalar(\"acc\", val_acc, step=epoch+1)\n",
    "            wtest.flush()\n",
    "    \n",
    "    template = \"\\nEpoch:{}/{} loss:{:0.5f}  acc:{:0.4f}  \"\n",
    "    template += \"val_loss:{:0.5f}  val_acc:{:0.4f} - {:0.0f}s(total: {:0.0f}s)\"\n",
    "    print(template.format(epoch+1, EPOCHS,\n",
    "                      loss.numpy(), acc.numpy(), \n",
    "                      val_loss, val_acc,\n",
    "                      perf_counter()-local_st, perf_counter()-global_st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "colab_type": "text",
    "id": "De0UlsaOcAE1"
   },
   "source": [
    "# Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "CNXN3EXFcAE5"
   },
   "source": [
    "<pre>\n",
    "1. Use <a href='https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16'>VGG-16</a> pretrained network without Fully Connected layers and initilize all the weights with Imagenet trained weights.\n",
    "2. After VGG-16 network without FC layers, don't use FC layers, use conv layers only as Fully connected layer. any FC layer can be converted to a CONV layer. This conversion will reduce the No of Trainable parameters in FC layers. For example, an FC layer with K=4096 that is looking at some input volume of size 77512 can be equivalently expressed as a CONV layer with F=7,P=0,S=1,K=4096. In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be 114096 since only a single depth column fits across the input volume, giving identical result as the initial FC layer. You can refer <a href='http://cs231n.github.io/convolutional-networks/#convert'>this</a> link to better understanding of using Conv layer in place of fully connected layers.\n",
    "3. Final architecture will be VGG-16 without FC layers(without top), 2 Conv layers identical to FC layers, 1 output layer for 16 class classification. <b>INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer</b>\n",
    "3. Train only last 2 Conv layers identical to FC layers, 1 output layer. Don't train the VGG-16 network. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "colab_type": "text",
    "id": "amKbfojfcAE-"
   },
   "source": [
    "# Model-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "N9AULF-PcAFC"
   },
   "source": [
    "<pre>\n",
    "1. Use same network as Model-2 '<b>INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer</b>' and train only Last 6 Layers of VGG-16 network, 2 Conv layers identical to FC layers, 1 output layer.\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
